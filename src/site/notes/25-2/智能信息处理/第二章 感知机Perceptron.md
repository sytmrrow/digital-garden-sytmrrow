---
{"dg-publish":true,"permalink":"/25-2/智能信息处理/第二章 感知机Perceptron/","dgPassFrontmatter":true,"created":"2025-06-23T16:42:20.860+08:00","updated":"2025-06-24T08:32:37.344+08:00"}
---

感知机是二类分类的线性分类模型，其输入为实例的<font color="#c00000">特征向量</font>，输入为实例的<font color="#c00000">类别</font>，取+1与-1二值。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。

感知机的学习旨在求出将训练数据进行线性划分的分离超平面。

导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型，感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。
是神经网络与支持向量机的基础。

## 一、感知机模型
![Pasted image 20250623164812.png](/img/user/Pasted%20image%2020250623164812.png)
感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型/线性分类器，即函数集合{f|f(x)=wx+b}

### 感知机解释
线性方程wx+b=0对应着特征空间中的一个超平面S，其中w是超平面的法向量，b是超平面的截距，这个超平面将特征空间划分为两个部分，位于两部分的点（特征向量）被分为正、负两类，因此超平面S也被称为分离超平面，如图。
![Pasted image 20250623165141.png](/img/user/Pasted%20image%2020250623165141.png)
## 二、感知机学习的策略
### 1.数据集的线性可分性
如果存在某个超平面能够将数据集中的所有正实例点和负实例点<font color="#c00000">完全正确</font>地划分到超平面的两侧，则称T为线性可分数据集，否则，则称数据集T为线性不可分。

### 2.感知机学习策略
假设数据集线性可分，那么我们需要找到这个能够将数据集完全划分正确的超平面，为了找到这样的超平面，即确定w与b的参数，我们需要为感知机的学习设定一个（经验）损失函数，并将这个损失函数极小化。

确定损失函数？
一个自然选择是分类错误的个数，但是这个不是参数w与b的连续可导函数，不易优化
另一个选择是<font color="#c00000">误分类点到超平面S的总距离</font>，这是感知机所采用的。
![Pasted image 20250623170111.png](/img/user/Pasted%20image%2020250623170111.png)
其中M为误分类点的集合。

感知机学习的策略就是在假设空间中选取使得损失函数式（2.4）最小的模型参数w,b，即感知机模型。

## 三、感知机学习算法
感知机学习问题已经转化为求解损失函数的最优化问题，最优化的方法是<font color="#c00000">随机梯度下降法</font>。具体算法包含原始形式和对偶形式，并且在训练数据可分条件下感知机算法具有收敛性。

### 1.感知机学习算法的原始形式
感知机算法是误分类驱动的，具体采用随机梯度下降法，首先，任意选取一个超平面w0,b0，然后用个梯度下降法不断地极小化目标函数，极小化的过程不是一次性使M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。
![Pasted image 20250623171243.png](/img/user/Pasted%20image%2020250623171243.png)
其中n（0<n<=1)为步长，在统计学习中又称为学习率。
这样可以通过迭代学习使得期待损失函数不断减小，直到为0。

算法如下：
（原始形式）直接更新特征空间中参数
（1）初始化w0与b0
（2）从训练数据集中选取点
（3）如果w为误分类点yi（wxi+b)<0，梯度下降优化
（4）转至（2），直至数据集中没有误分类点
![Pasted image 20250623172940.png](/img/user/Pasted%20image%2020250623172940.png)
感知机算法由于采用不同的初值或者<font color="#c00000">选取不同的误分类点，解可以不同</font>。

定理证明，误分类的次数是有上界的，经过有限次搜索可找到将训练数据完全正确分离的超平面，也就是说，当训练数据集线性可分的时候，感知机学习算法的原始形式是收敛的，但是感知机学习算法存在多解，这些解既依赖初值的选择，也依赖于迭代过程中误分类点的选择顺序，为了得到唯一的超平面，需要增加约束条件。
![Pasted image 20250624041828.png](/img/user/Pasted%20image%2020250624041828.png)
![Pasted image 20250623173549.png](/img/user/Pasted%20image%2020250623173549.png)
当训练集线性不可分时，感知机算法不收敛，迭代结果会发生震荡。

### 2.感知机学习算法的对偶形式
对偶形式的基本思想是，将w和b表示为实例xi和标记yi的线性组合形式，通过求解其系数而求得w和b。利用不同数据的影响力综合确定位置。
![Pasted image 20250623172955.png](/img/user/Pasted%20image%2020250623172955.png)

对偶形式中训练实例仅以内积的形式出现，为了方便，可以预先将训练集中实例间的内积算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵。

与原始形式一样，感知机的对偶形式也是收敛的存在多个解。


hoeffding不等式适用于<font color="#c00000">有界的随机变量</font>