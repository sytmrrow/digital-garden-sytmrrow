---
{"dg-publish":true,"permalink":"/25-2/智能信息处理/第六章 逻辑斯谛回归与最大熵模型/","dgPassFrontmatter":true,"created":"2025-06-23T17:45:51.629+08:00","updated":"2025-06-24T08:32:40.990+08:00"}
---

逻辑斯谛回归是统计学习中的经典分类方法。最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型。逻辑斯谛回归模型与最大熵模型都属于<font color="#c00000">对数线性模型</font>。
# 一、逻辑斯谛回归模型
## 1<span style="background:#fff88f">-逻辑斯谛分布</span>
![Pasted image 20250623175704.png](/img/user/Pasted%20image%2020250623175704.png)
![Pasted image 20250623175714.png](/img/user/Pasted%20image%2020250623175714.png)
分布函数属于逻辑斯谛函数，其图形是一条S形曲线，该曲线以点<font color="#c00000">（u,1/2)为中心对称</font>满足如下：![Pasted image 20250623175820.png](/img/user/Pasted%20image%2020250623175820.png)
曲线在中心附近增长速度较快，在两端增长速度较慢，形状参数<font color="#c00000">y的值越小，曲线在中心附近增长越快</font>。

## 2-二项逻辑斯谛回归模型
二项逻辑斯谛回归模型是一种分类模型，由条件概率分布P(Y|X)表示，形式为参数化的逻辑斯谛分布。随机变量X取值为实数，随机变量Y的取值为0/1，我们通过监督的方发生来估计模型参数。
![Pasted image 20250623180131.png](/img/user/Pasted%20image%2020250623180131.png)
逻辑斯谛回归比较P(1|X)与P(0|X)的大小，将实例x分到概率值较大的那一类。
有时为了方便将去权值向量与输入向量加以扩充可以得到简化的模型表示。
![Pasted image 20250623180341.png](/img/user/Pasted%20image%2020250623180341.png)

一个事件发生的几率（odds）是指该事件发生与不发生概率的比值，如果发生的概率是p，那么该事件的几率就是p/(1-p)，该事件的对数几率logit函数是logit p = log(p/1-p)
![Pasted image 20250623180628.png](/img/user/Pasted%20image%2020250623180628.png)

![Pasted image 20250623180920.png](/img/user/Pasted%20image%2020250623180920.png)
这里线性函数的值接近正无穷，概率值接近1，线性函数值接近负无穷概率接近0.

## 3.模型参数估计
可以应用极大似然估计法估计模型参数
问题变成了<font color="#c00000">对数似然函数为目标函数</font>的最优化问题，逻辑斯谛回归学习中通常采用的方法是<span style="background:#fff88f">梯度下降及拟牛顿法</span>

## 4.多项逻辑斯谛回归
可以将二类分类推广为多项逻辑斯谛回归模型，用于多类分类。
![Pasted image 20250623182851.png](/img/user/Pasted%20image%2020250623182851.png)

二项逻辑斯谛回归的参数估计法也可以推广到多项逻辑斯谛回归。


# 二、最大熵模型
最大熵模型由最大熵原理推导实现。

## 1.最大熵原理
最大熵原理是概率模型学习的一个准则。

最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，<font color="#c00000">熵最大的模型就是最好的模型</font>。通常用约束条件来确定概率模型的集合，所以最大熵原理可以表述为在给定的概率模型（分布）集合中寻找熵最大的模型

熵的表达式
![Pasted image 20250623183135.png](/img/user/Pasted%20image%2020250623183135.png)
<span style="background:#fff88f">当X服从均匀分布的时候熵最大</span>。

最大熵原理认为要选择的概率模型首先必须满足已有的约束条件，在没有更多信息的情况下，不确定的部分是等可能的，<font color="#c00000">最大熵原理通过熵的最大化来表示等可能性</font>，等可能性不易操作，而熵是一个可优化的数值。

通过满足条件的基础上求等概率的方法来估计概率分布就是使用了最大熵原理。

学习的目的是在所有可能的集合中选择最优的模型，而最大熵原理则给出了最优模型的一个选择方案。

## 2.最大熵模型的定义
最大熵元理论是统计学习的一般原理，将他应用到分类问题中得到最大熵模型。

![Pasted image 20250623183753.png](/img/user/Pasted%20image%2020250623183753.png)

## 3.最大熵模型的学习
最大熵模型的学习过程就是求解最大熵模型的过程，最大熵模型的学习过程可以形式化为约束最优化的问题。
最大化符合约束条件的模型的熵。根据解决最优化问题的习惯，将最大值问题转换为最小值问题。

需要将约束最优化的原始问题转换为无约束最优化的对偶问题，通过求解对偶问题求解原始问题。
最大熵模型的学习归结为对偶函数的极大化。

## 4.极大似然估计
对偶函数的极大化等价于最大熵模型的极大似然估计

最大熵模型与逻辑斯谛回归模型有类似的形式，它们又称为对数线性模型，模型的学习就是在给定的训练数据条件下对模型进行极大似然估计或者正则化的极大似然估计。

# 三、模型学习的最优化算法
逻辑斯蒂回归模型，最大熵模型学习归结为似然函数为目标函数的最优化问题，通常通过迭代算法求解。
从最优化的观点看，这时的目标函数是光滑的凸函数，因此多种方法都使用，保证能够找到全局最优解。常用的方法有改进的迭代尺度法、梯度下降法、牛顿法和拟牛顿法。牛顿法或拟牛顿法一般收敛速度更快。

## 1.改进的迭代尺度法
改进的迭代尺度法IIS是一种最大熵模型学习的最优化算法

目标是通过极大似然估计学习模型参数，即求对数似然函数的极大值

如果能够找到合适的参数使得下界提高，那么对数似然函数也会提高，然而存在多个变量，不易同时优化，IIS试图一次只优化其中一个变量，而固定其他变量

算法
![Pasted image 20250623201513.png](/img/user/Pasted%20image%2020250623201513.png)

牛顿法通过迭代求解

### 2.拟牛顿法
最大熵模型学习还可以应用牛顿法和拟牛顿法
![Pasted image 20250623201729.png](/img/user/Pasted%20image%2020250623201729.png)
