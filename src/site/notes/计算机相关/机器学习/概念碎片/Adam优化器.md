---
{"dg-publish":true,"permalink":"/计算机相关/机器学习/概念碎片/Adam优化器/","dgPassFrontmatter":true,"created":"2025-05-09T01:06:58.576+08:00","updated":"2025-05-09T02:16:34.703+08:00"}
---

一种改进版的[[计算机相关/机器学习/概念碎片/基础概念/梯度下降gradient descent\|梯度下降gradient descent]]算法，结合了两种经典优化器的优点：
[[计算机相关/机器学习/概念碎片/动量法Momentum\|动量法Momentum]]：加速收敛，减少震荡
[[计算机相关/机器学习/概念碎片/RMSProp优化器\|RMSProp优化器]]：根据每个参数的历史梯度调整学习率

Adam能够自动调整每个参数的学习率，还能够利用过去的梯度“记忆”加速优化，使它比普通的梯度下降收敛得更快、更稳。

# 核心思想
Adam更新参数时，同时考虑：
![Pasted image 20250509015456.png](/img/user/Pasted%20image%2020250509015456.png)

## Adam一阶矩（均值估计）
本质：历史梯度的滑动平均
作用：让更新过程<font color="#c00000">更平滑</font>，不被当前的单个大梯度扰动

## Adam二阶矩（方差估计）
本质：记录梯度的方差，用来给大梯度方向调小步长
作用：在震荡大的方向上<font color="#c00000">放慢步长</font>，避免发散

# 数学公式
![Pasted image 20250509020307.png](/img/user/Pasted%20image%2020250509020307.png)
# 优点
可以自动调整参数学习率、收敛速度快、对于稀疏梯度/非平稳目标效果好、几乎不用复杂调参

# 缺点
在某些任务上可能会过拟合（[[计算机相关/机器学习/概念碎片/基础概念/过拟合vs欠拟合#过拟合\|过拟合vs欠拟合#过拟合]]）
有时候不如[[计算机相关/机器学习/概念碎片/SGD\|SGD]]+学习率衰减找到更好的最终泛化能力